# 🧠 MemContext: 下一代多模态Agent记忆框架

<div align="center">

![License](https://img.shields.io/badge/license-Apache%202.0-blue)
![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![Status](https://img.shields.io/badge/Status-In%20Development-orange)
![Multi-modal](https://img.shields.io/badge/Modality-Audio%20|%20Video%20|%20Image%20|%20Document%20|%20Text-green)

**首个为 AI Agent 打造的全模态跨平台记忆框架**

*存万物 · 查万物 · 帧级定位 · 多平台支持*

[English](README.md) • [核心特性](#-核心特性) • [架构设计](#-架构设计) • [应用场景](#-应用场景) • [快速开始](#-快速开始)

</div>

---

## 📖 简介

**MemContext 致力于为 AI Agent 构建一个可持久化、高保真、可进化、跨平台的全模态记忆库。**

现有的 Agent Memory 框架多以文本为中心，往往难以原样保留视频、音频与文档等信息，进而带来多模态内容丢失、以及跨时间线回忆时的上下文错位。

MemContext 以**全模态原生**为设计原则，面向视频、音频与文档流等提供从**全模态输入、流式存储到帧级检索**的端到端能力：不论是规模化的长时素材库，还是画面中容易被忽略的细节线索，都能被可靠写入并在需要时精准找回。

同时，MemContext 深度适配 n8n、Coze、MCP、Skills 等主流生态，通过插件化机制赋能现有 Agent 工作流，实现跨平台的记忆能力集成。

---

## ✨ 核心特性

### 1. ♾️ 全模态存储与检索

**超越 Text-to-Text 的限制** MemContext 统一了异构数据的处理范式。

* **输入:** 统一处理视频、音频、图像、文档及文本。
* **多模态原生索引:** 采用“并行检索工作流”，直接对视觉和听觉信号建立向量索引，而非仅仅依赖其文字描述。
* **跨模态检索:** 支持“以图搜视”、“以音搜文”。

### 2. 🎞️ 无限流处理

**忘掉对有限上下文窗口的焦虑**

* 专为 **长上下文数据** 和 **24/7 持续运行** 的 Agent 设计。
* 支持无限时长的视频和连续音频流录入。
* **动态分块:** 内置连续性检查机制，在处理海量会议记录、影视剧集或监控流时，确保记忆上下文不发生断裂。

### 3. 🎯 0.1秒级时空精定位

**停止对时间戳的幻觉**

* 传统RAG与Memory系统只能告诉你“答案在哪个文件”，MemContext 能告诉你**“答案在第几分第几秒”**。
* **SOTA 级精度:** 能够以 **0.1 秒的精度** 检索并定位视频/音频片段。
* **价值:** 返回结果不再是臃肿的1GB文件，而是精确命中的0.1秒级关键片段，实现真正的“大海捞针”。

### 4. 🤖 多平台无缝集成

**开箱即用，深度兼容主流智能体平台**

* **支持 n8n 插件：** 为自动化工作流带来持久、可检索的多模态记忆。
* **支持 Coze 插件：** 让 Coze Agent 拥有跨会话的知识与上下文记忆延续能力。
* **支持 MCP:** 方便企业级智能体管理多用户、多会话的记忆数据。
* **支持 Claude Skills:** 为 Claude 助手场景赋能，大幅提升上下文理解深度和数据调用能力。
* **统一 API:** 无论您使用哪种平台，都可通过标准 RESTful 或 SDK 轻松完成集成。
* **价值:** 让您的多模态记忆能力真正“即插即用”，跨越平台、场景和生态无障碍适配。

---

## 🏗️ 架构设计

MemContext 借鉴了人类大脑的认知过程以及操作系统的存储结构，采用了分层存储架构配合双路检索引擎。

<div align="center">
  <img src="assets/MemContext_Workflow_v0.png" alt="MemContext Architecture Workflow" width="100%">
</div>

### 🧠 记忆生命周期

数据流经记忆管道全过程：

1. **多模态输入:** 统一处理器从异构数据中提取多维特征。
2. **STM (短期记忆):** 处理即时上下文流，执行 Embedding 计算与初步过滤。
3. **MTM (中期记忆):** 基于会话的缓冲区，引入**热度计算算法 (Heat Calculation)**。系统根据访问频次 ($N_{visit}$)、交互深度 ($L_{interaction}$) 和新近度 ($R_{recency}$) 动态评判数据价值。
4. **LTM (长期记忆):** 高热度信息“结晶”为永久存储（用户画像 & 知识库），低价值数据则被从工作记忆中淘汰。

### 🔍 精准检索引擎

支持自然语言、图像、视频片段等多种查询形式：

1. **语义过滤:** 快速从 MTM 中筛选相关会话上下文。
2. **向量相似度匹配:** 在 LTM 知识库中进行深度全库扫描。
3. **时空定位:** 执行高精度的帧级定位，锁定具体数据片段。
4. **结果聚合:** 将检索到的视频切片、背景知识和对话历史等上下文融合为结构化的上下文列表返回给Agent。

---

## 🚀 应用场景

### 1.个性化 AI Agent

* **痛点:** 现在的 Chatbot 聊完就忘。你上周说“我在减肥”，今天它还给你推荐高热量餐厅；你三个月前提到过“讨厌香菜”，它现在完全不记得。
* **MemContext 的记忆体现:**
  * **LTM (长期记忆 - 画像进化):** 当你曾在多次对话中通过语音或文本流露过对“低糖饮食”的偏好，系统会自动将这一**高热度信息**结晶化，写入 `User Profile`。
  * **记忆回溯:** 当你今天问“中午吃什么？”，Agent 不会随机推荐，而是调用 **LTM**：“记得你正在**生酮饮食期间（长期记忆）**，且上周五（中期记忆）你提到想试那家新开的沙拉店，但当时排队太长没去成，要不今天试试？”
  * **价值:** 从“一问一答”进化为**跨越时间周期的主动关怀**。

### 2.长周期项目的一体化大脑

* **痛点:** 在一个长达半年的项目中，早期的会议录音、白板草图和现在的代码文档是割裂的。你很难问 AI：“我们现在的方案和两个月前第一次头脑风暴时的想法有什么偏差？”
* **MemContext 的记忆体现:**
  * **MTM (中期记忆 - 热度召回):** 虽然“第一次头脑风暴”是两个月前的 Session，但因为那是项目起点，**访问频次 ($N_{visit}$)** 高，它在 MTM 中一直保持“温热”状态，未被遗忘。
  * **跨模态验证:** Agent 能够同时调取两个月前的白板照片（视觉记忆）和当时的会议录音（听觉记忆），与现在的设计文档（文本输入）进行比对。
  * **价值:** 连接孤立的时间切片，防止项目“初心”遗忘，提供跨越数月的一致性监督。

### 3.伴随式学习导师

* **痛点:** 传统教育 AI 不知道你的学习曲线。它不知道你 5 分钟前刚在某个知识点上卡住了，也不知道你 3 天前在另一个相关视频里已经学懂了基础概念。
* **MemContext 的记忆体现:**
  * **STM (短期记忆 - 上下文感知):** 此时此刻，当你对着题目皱眉（摄像头捕捉到表情或听到叹气），STM 捕捉到你的困惑。
  * **知识关联:** Agent 检索 **LTM** 发现你曾在一个月前的视频课程中，在“微积分链式法则”这个点上停留了很长时间（高交互深度 $L_{interaction}$）。
  * **主动干预:** “看起来你在这个步骤卡住了。这和我们上个月（记忆回溯）复习的‘链式法则’很像，还记得当时那个红色的球体演示动画吗？我帮你调出来。”
  * **价值:** 建立学习历史的连续性，提供**因材施教的个性化辅助**。

### 4. 智能家庭的“管家记忆”

* **痛点:** 智能音箱只能执行指令，没有家庭历史感。
* **MemContext 的记忆体现:**
  * **情节记忆:** 用户：“我想看上次过年大家一起包饺子时的那个视频，好像是奶奶在教我捏褶子。”
  * **复杂语义定位:** 系统不是去搜文件名，而是理解“过年（时间范畴）”、“大家（多人物识别）”、“包饺子（动作识别）”、“奶奶教我（交互关系）”。
  * **高精度呈现:** 直接定位并播放**那段 30 秒的温馨片段**，而不是把整整 2 小时的家庭录像丢给你。
  * **价值:** 存储家庭的高光时刻，而不仅仅是存储冷冰冰的文件。

---

## 📋 前置要求

- **Python 3.10+**
- **flask>=2.0.0,<3.0.0**

---

## ⚡ 快速开始

1) 创建Python虚拟环境

```bash
conda create -n memcontext python=3.10 -y
conda activate memcontext
```

2) 安装依赖
   在根目录下导入requirements

```bash
pip install -r requirements.txt
```

3) 配置密钥

在memdemo目录下新建.env文件，添加LLM API KEY等秘钥
示例如下：

```
#LLM API配置（用于内容分析和智能对话）
LLM_API_KEY=YOUR-API-KEY
LLM_BASE_URL=https://ark.cn-beijing.volces.com/api/v3
LLM_MODEL=doubao-seed-1-6-flash-250828
#向量化Embedding API配置（用于向量数据库）
EMBEDDING_API_KEY=YOUR-API-KEY
EMBEDDING_BASE_URL=https://ark.cn-beijing.volces.com/api/v3
EMBEDDING_MODEL=doubao-embedding-large-text-250515
#硅基流动API配置（用于音频理解模型）
SILICONFLOW_API_KEY=YOUR-API-KEY
SILICONFLOW_MODEL=TeleAI/TeleSpeechASR
#是否启用音频转换配置
ENABLE_AUDIO_TRANSCRIPTION=true
```

4) 运行具备记忆功能的AI对话 Demo

4.1）可使用flask版本
```bash
cd memdemo
python app.py        # Flask 版本 Demo，默认端口 5019
```


4.2）使用fastAPI版本
如果你使用的是 **FastAPI 版本 Demo**（`app_fastAPI.py`），请先确保本机已经启动 Redis 服务（用于限流与会话存储）。示例（使用 Docker 启动 Redis）：

```bash
docker run -d --name memcontext-redis -p 6379:6379 redis:7-alpine

cd memdemo
python app_fastAPI.py   # FastAPI 版本 Demo，默认端口 5019
```

在浏览器中打开 http://localhost:5019/ 可以看到登录界面，输入用户名即可与 AI 对话，体验记忆功能。
![memdemo_login](assets/memdemo_login.png)

## 🤝 贡献指南

MemContext 的核心功能正在紧锣密鼓地开发中。如果你对多模态 Agent Memory 系统感兴趣，欢迎 Star 并关注我们的进展。

## 📜 协议

Apache 2.0
