# 🧠 ContextBase: 下一代多模态Agent记忆框架

<div align="center">

![License](https://img.shields.io/badge/license-Apache%202.0-blue)
![Python](https://img.shields.io/badge/python-3.9%2B-blue)
![Status](https://img.shields.io/badge/Status-In%20Development-orange)
![Multi-modal](https://img.shields.io/badge/Modality-Audio%20|%20Video%20|%20Image%20|%20Document%20|%20Text-green)

**首个为 AI Agent 打造的“全模态”记忆框架**

*存万物 · 查万物 · 帧级定位*

[English](README.md) • [核心特性](#-核心特性) • [架构设计](#-架构设计) • [应用场景](#-应用场景)

</div>

---

## 📖 简介

**ContextBase 致力于为 AI Agent 构建一个持久化、高保真、可进化的“第二大脑”。**

现有的 Memory 框架大多强行将丰富多彩的物理世界压缩为纯文本，导致了视觉细节丢失和上下文的时序错位。**ContextBase 拒绝这种降维压缩。** 

ContextBase是**多模态原生 (Multi-modal Native)** 的记忆框架，旨在以原始的高保真形式处理视频、音频和文档流。无论是数百小时的影视素材库，还是画面背景里隐藏的一个细微彩蛋，ContextBase 都能完成从**全模态数据输入**、**流式存储**到**帧级检索**的高精度闭环。

我们构建的不是静态数据库，而是 Agent 的**原生时空感知**。

---

## ✨ 核心特性

### 1. ♾️ 全模态存储与检索
**超越 Text-to-Text 的限制** ContextBase 统一了异构数据的处理范式。
* **输入:** 统一处理视频、音频、图像、文档及文本。
* **多模态原生索引:** 采用“并行检索工作流”，直接对视觉和听觉信号建立向量索引，而非仅仅依赖其文字描述。
* **跨模态检索:** 支持“以图搜视”、“以音搜文”。

### 2. 🎞️ 无限流处理
**忘掉对有限上下文窗口的焦虑**
* 专为 **长上下文数据** 和 **24/7 持续运行** 的 Agent 设计。
* 支持无限时长的视频和连续音频流录入。
* **动态分块:** 内置连续性检查机制，在处理海量会议记录、影视剧集或监控流时，确保记忆上下文不发生断裂。

### 3. 🎯 0.1秒级时空精定位
**停止对时间戳的幻觉**
* 传统RAG与Memory系统只能告诉你“答案在哪个文件”，ContextBase 能告诉你**“答案在第几分第几秒”**。
* **SOTA 级精度:** 能够以 **0.1 秒的精度** 检索并定位视频/音频片段。
* **价值:** 返回结果不再是臃肿的1GB文件，而是精确命中的0.1秒级关键片段，实现真正的“大海捞针”。

---

## 🏗️ 架构设计

ContextBase 借鉴了人类大脑的认知过程以及操作系统的存储结构，采用了分层存储架构配合双路检索引擎。

<div align="center">
  <img src="ContextBase_Workflow_v0.png" alt="ContextBase Architecture Workflow" width="100%">
</div>

### 🧠 记忆生命周期
数据流经仿生的处理管道：
1.  **多模态输入:** 统一处理器从异构数据中提取多维特征。
2.  **STM (短期记忆):** 处理即时上下文流，执行 Embedding 计算与初步过滤。
3.  **MTM (中期记忆):** 基于会话的缓冲区，引入**热度计算算法 (Heat Calculation)**。系统根据访问频次 ($N_{visit}$)、交互深度 ($L_{interaction}$) 和新近度 ($R_{recency}$) 动态评判数据价值。
4.  **LTM (长期记忆):** 高热度信息“结晶”为永久存储（用户画像 & 知识库），低价值数据则被从工作记忆中淘汰。

### 🔍 精准检索引擎
支持自然语言、图像、视频片段等多种查询形式：
1.  **语义过滤:** 快速从 MTM 中筛选相关会话上下文。
2.  **向量相似度匹配:** 在 LTM 知识库中进行深度全库扫描。
3.  **时空定位:** 执行高精度的帧级定位，锁定具体数据片段。
4.  **结果聚合:** 将检索到的视频切片、背景知识和对话历史等上下文融合为结构化的上下文列表返回给Agent。

---

## 🚀 应用场景

### ✂️ 智能视频剪辑助手
* **场景:** 纪录片或综艺剪辑师面对数 TB 的原始素材 (B-Roll)，需要快速找到“主角在海边大笑，且背景有海鸥叫声”的镜头，用于拼接情感高潮段落。
* **ContextBase 方案:** 
    * **极速粗筛:** 利用语义理解快速锁定包含“海边”和“笑”的视频文件。
    * **精准刀法:** 结合音频模态（识别海鸥声）和视觉模态（识别主角在海边大笑动作），利用 **0.1s 时空精定位** 能力，直接输出该镜头的 `Inpoint` 和 `Outpoint`时间码。
    * **结果:** 剪辑师不再需要手动拖动进度条寻找，Agent 直接生成可用的素材剪辑列表（EDL）。

### 🖼️ 下一代情境感知相册
* **场景:** 用户不仅仅想找“某张照片”，而是想找回“一段记忆”。例如：“帮我找两年前我们在那家放着爵士乐的咖啡馆，讨论创业计划时拍的照片。”
* **ContextBase 方案:** 
    * **跨模态联想:** 这是一个典型的多模态复合查询。系统不仅检索图像内容（咖啡馆），还同时检索当时的背景环境音（爵士乐音频流）以及可能的对话记录（创业计划文本）。
    * **记忆唤醒:** ContextBase 将碎片化的图像与当时的声音、对话链接起来，不仅仅返回一张静态图片，还能重现当时的情景。

### 🕵️‍♂️ 法律与证据分析
* **场景:** Agent 需要在长达 50 小时的证人证词视频中寻找前后矛盾之处。
* **ContextBase 方案:** Agent 可以查询特定的动作或陈述（例如“找出所有嫌疑人看手表的时刻”），系统将返回精确到 0.1s 的视频切片，即刻验证证据。

### 🎓 在线教育与长视频知识提取
* **场景:** 学生面对一个学期长达 100 小时的录播课程，想复习教授讲解“Transformer 架构中的 Attention 机制”的具体推导过程。
* **ContextBase 方案:** 
    * **无限流处理:** 支持对整个学期的课程视频进行索引。
    * **知识定位:** 学生无需观看整节课，系统直接定位到教授在黑板上画出 Attention 公式的 **2 分钟精准片段**，并同步展示该时刻的语音转写字幕，极大提高学习效率。

### 👓 穿戴式 AI 与第一视角记忆
* **场景:** 佩戴智能眼镜的用户询问 Agent：“我把 AirPods 落在哪里了？”
* **ContextBase 方案:** 系统对第一视角视频流进行实时索引。通过**视觉物体定位 (Visual Object Grounding)** 技术，系统不依赖文本标签，而是直接检索“AirPods”的视觉特征，回溯并锁定用户最后一次与其交互的画面帧，提供确切的位置快照。


---

## ⚡ 快速开始

*(代码库整理中，即将开放)*

## 🤝 贡献指南

ContextBase 的核心功能正在紧锣密鼓地开发中。如果你对多模态、RAG 或 Agent Memory 系统感兴趣，欢迎 Star 并关注我们的进展。

## 📜 协议

Apache 2.0